{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping-Lab\" data-toc-modified-id=\"Web-Scraping-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web Scraping Lab</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Useful-Resources\" data-toc-modified-id=\"Useful-Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Useful Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-of-all,-gathering-our-tools.\" data-toc-modified-id=\"First-of-all,-gathering-our-tools.-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>First of all, gathering our tools.</a></span></li><li><span><a href=\"#Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:\" data-toc-modified-id=\"Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:-1.0.1.2\"><span class=\"toc-item-num\">1.0.1.2&nbsp;&nbsp;</span>Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:</a></span></li><li><span><a href=\"#Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.\" data-toc-modified-id=\"Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.-1.0.1.3\"><span class=\"toc-item-num\">1.0.1.3&nbsp;&nbsp;</span>Display the names of the trending developers retrieved in the previous step.</a></span></li><li><span><a href=\"#Challenge-2---Display-the-trending-Python-repositories-in-GitHub\" data-toc-modified-id=\"Challenge-2---Display-the-trending-Python-repositories-in-GitHub-1.0.1.4\"><span class=\"toc-item-num\">1.0.1.4&nbsp;&nbsp;</span>Challenge 2 - Display the trending Python repositories in GitHub</a></span></li><li><span><a href=\"#Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page\" data-toc-modified-id=\"Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page-1.0.1.5\"><span class=\"toc-item-num\">1.0.1.5&nbsp;&nbsp;</span>Challenge 3 - Display all the image links from Walt Disney wikipedia page</a></span></li><li><span><a href=\"#Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.\" data-toc-modified-id=\"Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.-1.0.1.6\"><span class=\"toc-item-num\">1.0.1.6&nbsp;&nbsp;</span>Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python.</a></span></li><li><span><a href=\"#Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point\" data-toc-modified-id=\"Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point-1.0.1.7\"><span class=\"toc-item-num\">1.0.1.7&nbsp;&nbsp;</span>Challenge 5 - Number of Titles that have changed in the United States Code since its last release point</a></span></li><li><span><a href=\"#Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names\" data-toc-modified-id=\"Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names-1.0.1.8\"><span class=\"toc-item-num\">1.0.1.8&nbsp;&nbsp;</span>Challenge 6 - A Python list with the top ten FBI's Most Wanted names</a></span></li><li><span><a href=\"#Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org\" data-toc-modified-id=\"Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org-1.0.1.9\"><span class=\"toc-item-num\">1.0.1.9&nbsp;&nbsp;</span>Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org</a></span></li><li><span><a href=\"#Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk\" data-toc-modified-id=\"Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk-1.0.1.10\"><span class=\"toc-item-num\">1.0.1.10&nbsp;&nbsp;</span>Challenge 8 - A list with the different kind of datasets available in data.gov.uk</a></span></li><li><span><a href=\"#Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe\" data-toc-modified-id=\"Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe-1.0.1.11\"><span class=\"toc-item-num\">1.0.1.11&nbsp;&nbsp;</span>Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe</a></span></li></ul></li><li><span><a href=\"#Stepping-up-the-game\" data-toc-modified-id=\"Stepping-up-the-game-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Stepping up the game</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe-1.0.2.2\"><span class=\"toc-item-num\">1.0.2.2&nbsp;&nbsp;</span>Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.-1.0.2.3\"><span class=\"toc-item-num\">1.0.2.3&nbsp;&nbsp;</span>Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe.</a></span></li><li><span><a href=\"#Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.\" data-toc-modified-id=\"Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.-1.0.2.4\"><span class=\"toc-item-num\">1.0.2.4&nbsp;&nbsp;</span>Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city.</a></span></li><li><span><a href=\"#Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.-1.0.2.5\"><span class=\"toc-item-num\">1.0.2.5&nbsp;&nbsp;</span>Challenge 14 - Book name,price and stock availability as a pandas dataframe.</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Web Scraping\n",
    "\n",
    "Encontrarás en este cuaderno algunos ejercicios de web scraping para practicar tus habilidades de scraping usando `requests` y `Beautiful Soup`.\n",
    "\n",
    "**Consejos:**\n",
    "\n",
    "- Verifica el [código de estado de la respuesta](https://http.cat/) para cada solicitud para asegurarte de haber obtenido el contenido previsto.\n",
    "- Observa el código HTML en cada solicitud para entender el tipo de información que estás obteniendo y su formato.\n",
    "- Busca patrones en el texto de respuesta para extraer los datos/información solicitados en cada pregunta.\n",
    "- Visita cada URL y echa un vistazo a su fuente a través de Chrome DevTools. Necesitarás identificar las etiquetas HTML, nombres de clases especiales, etc., utilizados para el contenido HTML que se espera extraer.\n",
    "- Revisa los selectores CSS.\n",
    "\n",
    "### Recursos Útiles\n",
    "- Documentación de la [biblioteca Requests](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Doc de Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Lista de códigos de estado HTTP](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [Conceptos básicos de HTML](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [Conceptos básicos de CSS](https://www.cssbasics.com/#page_start)\n",
    "\n",
    "#### Primero que todo, reuniendo nuestras herramientas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Nuevamente, recuerda limitar tu salida antes de la entrega para que tu código no se pierda en la salida.**\n",
    "\n",
    "#### Desafío 1 - Descargar, analizar (usando BeautifulSoup) e imprimir el contenido de la página de Desarrolladores en Tendencia de GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/trending/developers'\n",
    "github_html=requests.get(url).text\n",
    "soup = BeautifulSoup(github_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain4j',\n",
       " 'Natik Gadzhi',\n",
       " 'Vik Paruchuri',\n",
       " 'jxxghp',\n",
       " 'Marcus Schiesser',\n",
       " 'Evan Wallace',\n",
       " 'Yair Morgenstern',\n",
       " 'Norman Maurer',\n",
       " 'Martin von Zweigbergk',\n",
       " 'Emil Ernerfeldt',\n",
       " 'Keren Zhou',\n",
       " 'Riad Benguella',\n",
       " 'Xuan Son Nguyen',\n",
       " '三咲雅 · Misaki Masa',\n",
       " 'Massimiliano Pippi',\n",
       " 'Steve Macenski',\n",
       " 'john verzani',\n",
       " 'Sebastian Raschka',\n",
       " 'Mohammed Abu Aboud',\n",
       " 'Azure SDK Bot',\n",
       " 'Robin Malfait',\n",
       " 'JJ Kasper',\n",
       " 'Henry Heng',\n",
       " 'Miguel Ángel Durán',\n",
       " 'Matthias Seitz']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "a = soup.find_all(class_='h3 lh-condensed')\n",
    "for item in a:\n",
    "    result = item.find_all('a')\n",
    "    names.append(result[0].get_text().strip())\n",
    "    \n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muestra los nombres de los desarrolladores en tendencia recuperados en el paso anterior.\n",
    "\n",
    "Tu salida debe ser una lista de Python con los nombres de los desarrolladores. Cada nombre no debe contener ninguna etiqueta HTML.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. Descubre la etiqueta HTML y los nombres de clase usados para los nombres de los desarrolladores. Puedes lograr esto usando Chrome DevTools.\n",
    "\n",
    "1. Usa BeautifulSoup para extraer todos los elementos HTML que contienen los nombres de los desarrolladores.\n",
    "\n",
    "1. Utiliza técnicas de manipulación de cadenas para reemplazar espacios en blanco y saltos de línea (es decir, `\\n`) en el *texto* de cada elemento HTML. Usa una lista para almacenar los nombres limpios.\n",
    "\n",
    "1. Imprime la lista de nombres.\n",
    "\n",
    "Tu salida debería lucir como abajo (con nombres diferentes):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain4j',\n",
       " 'Natik Gadzhi',\n",
       " 'Vik Paruchuri',\n",
       " 'jxxghp',\n",
       " 'Marcus Schiesser',\n",
       " 'Evan Wallace',\n",
       " 'Yair Morgenstern',\n",
       " 'Norman Maurer',\n",
       " 'Martin von Zweigbergk',\n",
       " 'Emil Ernerfeldt',\n",
       " 'Keren Zhou',\n",
       " 'Riad Benguella',\n",
       " 'Xuan Son Nguyen',\n",
       " '三咲雅 · Misaki Masa',\n",
       " 'Massimiliano Pippi',\n",
       " 'Steve Macenski',\n",
       " 'john verzani',\n",
       " 'Sebastian Raschka',\n",
       " 'Mohammed Abu Aboud',\n",
       " 'Azure SDK Bot',\n",
       " 'Robin Malfait',\n",
       " 'JJ Kasper',\n",
       " 'Henry Heng',\n",
       " 'Miguel Ángel Durán',\n",
       " 'Matthias Seitz']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "a = soup.find_all(class_='h3 lh-condensed')\n",
    "for item in a:\n",
    "    result = item.find_all('a')\n",
    "    names.append(result[0].get_text().strip())\n",
    "    \n",
    "names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 2 - Mostrar los repositorios de Python en tendencia en GitHub\n",
    "\n",
    "Los pasos para resolver este problema son similares al anterior, excepto que necesitas encontrar los nombres de los repositorios en lugar de los nombres de los desarrolladores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'\n",
    "datos2 = requests.get(f\"{url2}\").text\n",
    "soup2 = BeautifulSoup(datos2, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cover-agent',\n",
       " 'khoj',\n",
       " 'searxng',\n",
       " 'MiniCPM-V',\n",
       " 'CVE-2024-21683-RCE',\n",
       " 'omniglue',\n",
       " 'ungoogled-chromium',\n",
       " 'bisheng',\n",
       " 'nvda',\n",
       " 'ragflow',\n",
       " 'seismometer',\n",
       " 'core',\n",
       " 'accelerate',\n",
       " 'taipy',\n",
       " 'corenet',\n",
       " 'LaVague',\n",
       " 'gpt-researcher',\n",
       " 'krita-ai-diffusion',\n",
       " 'azure-sdk-for-python',\n",
       " 'unsloth',\n",
       " 'TagStudio',\n",
       " 'pathway',\n",
       " 'Python',\n",
       " 'Python-100-Days',\n",
       " 'sqlmesh']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reponames = []\n",
    "a = soup2.find_all(class_='h3 lh-condensed')\n",
    "for item in a:\n",
    "    result = item.find_all('a')\n",
    "    elementos = result[0].get_text().strip().split(' ')\n",
    "    reponames.append(elementos[-1])\n",
    "\n",
    "\n",
    "reponames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 3 - Mostrar todos los enlaces de imágenes de la página de Wikipedia de Walt Disney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "disney = requests.get(f\"{url3}\").text\n",
    "soup3 = BeautifulSoup(disney, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https:/static/images/icons/wikipedia.png',\n",
       " 'https:/static/images/mobile/copyright/wikipedia-wordmark-en.svg',\n",
       " 'https:/static/images/mobile/copyright/wikipedia-tagline-en.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Nuvola_apps_kaboodle.svg/16px-Nuvola_apps_kaboodle.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/20px-Commons-logo.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/23px-Wikiquote-logo.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/26px-Wikisource-logo.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Mickey_Mouse_colored_%28head%29.svg/20px-Mickey_Mouse_colored_%28head%29.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " 'https:/static/images/footer/wikimedia-button.png',\n",
       " 'https:/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encontrar todas las etiquetas de imagen\n",
    "images = soup3.find_all('img')\n",
    "\n",
    "# meteremos en esta lista\n",
    "image_links = []\n",
    "\n",
    "\n",
    "for image in images:\n",
    "    # sacar el atributo 'src' de cada etiqueta de imagen\n",
    "    src = image.get('src')\n",
    "    if src:\n",
    "        # asegurar que el enlace sea completo\n",
    "        if not src.startswith('http'):\n",
    "            src = 'https:' + src\n",
    "        image_links.append(src)\n",
    "\n",
    "\n",
    "image_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 4 - Recuperar todos los enlaces a páginas en Wikipedia que se refieren a algún tipo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "python = requests.get(f\"{url4}\").text\n",
    "soup4 = BeautifulSoup(python, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " '/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " '/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " '/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://az.wikipedia.org/wiki/Python_(d%C9%99qiql%C9%99%C5%9Fdirm%C9%99)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://tr.wikipedia.org/wiki/Python_(anlam_ayr%C4%B1m%C4%B1)',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://en.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/wiki/Talk:Python',\n",
       " 'https://en.wikipedia.org/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=history',\n",
       " 'https://en.wikipedia.org/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=history',\n",
       " 'https://en.wikipedia.org/wiki/Special:WhatLinksHere/Python',\n",
       " 'https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Python',\n",
       " '/w/index.php?title=Python&oldid=1213626445',\n",
       " '/w/index.php?title=Python&action=info',\n",
       " '/w/index.php?title=Special:CiteThisPage&page=Python&id=1213626445&wpFormIdentifier=titleform',\n",
       " '/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FPython',\n",
       " '/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FPython',\n",
       " '/w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen',\n",
       " '/w/index.php?title=Python&printable=yes',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://en.wiktionary.org/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit&section=1',\n",
       " 'https://en.wikipedia.org/wiki/Pythonidae',\n",
       " 'https://en.wikipedia.org/wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(mythology)',\n",
       " '/w/index.php?title=Python&action=edit&section=2',\n",
       " 'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
       " '/w/index.php?title=Python&action=edit&section=3',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org/wiki/Python_(painter)',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Byzantium',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Catana',\n",
       " 'https://en.wikipedia.org/wiki/Python_Anghelo',\n",
       " '/w/index.php?title=Python&action=edit&section=4',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Efteling)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/w/index.php?title=Python&action=edit&section=5',\n",
       " 'https://en.wikipedia.org/wiki/Python_(automobile_maker)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Ford_prototype)',\n",
       " '/w/index.php?title=Python&action=edit&section=6',\n",
       " 'https://en.wikipedia.org/wiki/Python_(missile)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(nuclear_primary)',\n",
       " 'https://en.wikipedia.org/wiki/Colt_Python',\n",
       " '/w/index.php?title=Python&action=edit&section=7',\n",
       " 'https://en.wikipedia.org/wiki/Python_(codename)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(film)',\n",
       " 'https://en.wikipedia.org/wiki/Monty_Python',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Monty)_Pictures',\n",
       " '/w/index.php?title=Python&action=edit&section=8',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=1213626445',\n",
       " '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# una lista\n",
    "python_links = []\n",
    "\n",
    "# encontrar todos los enlaces que contienen \"Python\"\n",
    "for link in soup4.find_all('a', href=True):\n",
    "    href = link.get('href')\n",
    "    if 'Python' in link.text or 'Python' in href:\n",
    "        full_url = f\"https://en.wikipedia.org{href}\" if href.startswith('/wiki/') else href\n",
    "        python_links.append(full_url)\n",
    "\n",
    "\n",
    "python_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 5 - Número de Títulos que han cambiado en el Código de los Estados Unidos desde su último punto de lanzamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "uscode_html = requests.get(url).text\n",
    "soup = BeautifulSoup(uscode_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 26 - Internal Revenue Code',\n",
       " 'Title 42 - The Public Health and Welfare']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lista para meter los datos\n",
    "changed_titles = []\n",
    "\n",
    "# aqui los compañeros nos ayudan con lo de la class\n",
    "changed_title_elements = soup.find_all('div', class_='usctitlechanged')\n",
    "\n",
    "\n",
    "for element in changed_title_elements:\n",
    "    title_text = element.get_text(strip=True)\n",
    "    changed_titles.append(title_text)\n",
    "\n",
    "changed_titles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 6 - Una lista de Python con los diez nombres más buscados por el FBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'https://en.wikipedia.org/wiki/FBI_Ten_Most_Wanted_Fugitives'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = requests.get(f\"{url7}\").text\n",
    "soup7 = BeautifulSoup(wanted, 'html.parser')\n",
    "wantedtag = soup7.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alexis Flores',\n",
       " 'Bhadreshkumar Chetanbhai Patel',\n",
       " 'Alejandro Castillo',\n",
       " 'Arnoldo Jimenez',\n",
       " 'Yulan Adonay Archaga Carias',\n",
       " 'Ruja Ignatova',\n",
       " 'Omar Alexander Cardenas',\n",
       " 'Wilver Villegas-Palomino',\n",
       " 'Donald Eugene Fields II',\n",
       " \"Vitel'Homme Innocent\"]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/FBI_Ten_Most_Wanted_Fugitives'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#la tabla que contiene los nombres\n",
    "table = soup.find('table', class_='wikitable')\n",
    "# extraer los nombres de la tabla\n",
    "names = []\n",
    "for row in table.find_all('tr')[1:]:  # ignoramos la primera fila que es el encabezado\n",
    "    name = row.find('th').text.strip()\n",
    "    names.append(name)\n",
    "\n",
    "names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 7 - Listar todos los nombres de idiomas y el número de artículos relacionados en el orden en que aparecen en wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = requests.get(f\"{url8}\").text\n",
    "soup8 = BeautifulSoup(languages, 'html.parser')\n",
    "langlist = soup8.find_all(\"div\", {\"class\": f\"central-featured-lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('English', '6796000'),\n",
       " ('Русский', '1969000'),\n",
       " ('日本語', '1407000'),\n",
       " ('Español', '1938000'),\n",
       " ('Deutsch', '2891000'),\n",
       " ('Français', '2598000'),\n",
       " ('Italiano', '1853000'),\n",
       " ('中文', '1409000'),\n",
       " ('فارسی', '۹۹۵۰۰۰'),\n",
       " ('Português', '1120000')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_articles = []\n",
    "languages = requests.get(f\"{url8}\")\n",
    "languages.encoding = 'uft-8'\n",
    "soup7 = BeautifulSoup(languages.text)\n",
    "langlist = soup7.find_all(\"div\", {\"class\": f\"central-featured-lang\"})\n",
    "for lang in langlist:\n",
    "        language = lang.find(\"strong\").text.strip()\n",
    "        articles = ''.join(filter(str.isdigit, lang.find(\"small\").text.strip()))\n",
    "        language_articles.append((language, articles))\n",
    "\n",
    "language_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 8 - Una lista con los diferentes tipos de conjuntos de datos disponibles en data.gov.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url82 = 'https://data.gov.uk/'\n",
    "dats = requests.get(f\"{url82}\")\n",
    "soup8 = BeautifulSoup(dats.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cookies on GOV.UK', 'Data topics', 'Support links']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encontrar todas las etiquetas <h2> que contienen los nombres de los tipos de conjuntos de datos\n",
    "dataset_types = soup.find_all('h2')\n",
    "\n",
    "# aqui meteremos\n",
    "dataset_types_list = []\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    dataset_types_list.append(dataset_type.text)\n",
    "\n",
    "dataset_types_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 9 - Los 10 idiomas con más hablantes nativos almacenados en un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "tenlang = requests.get(url9)\n",
    "soup9 = BeautifulSoup(tenlang.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mandarin Chinese', '941', 'Sino-Tibetan', 'Sinitic'],\n",
       " ['Spanish', '486', 'Indo-European', 'Romance'],\n",
       " ['English', '380', 'Indo-European', 'Germanic'],\n",
       " ['Hindi', '345', 'Indo-European', 'Indo-Aryan'],\n",
       " ['Bengali', '237', 'Indo-European', 'Indo-Aryan'],\n",
       " ['Portuguese', '236', 'Indo-European', 'Romance'],\n",
       " ['Russian', '148', 'Indo-European', 'Balto-Slavic'],\n",
       " ['Japanese', '123', 'Japonic', 'Japanese'],\n",
       " ['Yue Chinese', '86', 'Sino-Tibetan', 'Sinitic'],\n",
       " ['Vietnamese', '85', 'Austroasiatic', 'Vietic']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#meteremos aqui\n",
    "langs = []\n",
    "\n",
    "langtable = soup9.find(\"table\",class_=\"wikitable\")\n",
    "langrows = langtable.find_all(\"tr\")\n",
    "\n",
    "for row in langrows[1:11]:\n",
    "        col=row.find_all(\"td\")\n",
    "        lang=col[0].text.strip()\n",
    "        speakers=col[1].text.strip()\n",
    "        langfamily=col[2].text.strip()\n",
    "        branch=col[3].text.strip()\n",
    "        langs.append([lang,speakers,langfamily,branch])\n",
    "langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subiendo el nivel\n",
    "#### Desafío 10 - La información de los 20 últimos terremotos (fecha, hora, latitud, longitud y nombre de la región) por el EMSC como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Hora</th>\n",
       "      <th>Latitud</th>\n",
       "      <th>Longitud</th>\n",
       "      <th>Región</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>12:02:47</td>\n",
       "      <td>-37.96</td>\n",
       "      <td>-74.00</td>\n",
       "      <td>OFF COAST OF CENTRAL CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>11:52:45</td>\n",
       "      <td>-4.79</td>\n",
       "      <td>133.90</td>\n",
       "      <td>IRIAN JAYA REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>10:16:33</td>\n",
       "      <td>18.98</td>\n",
       "      <td>-65.48</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>10:10:39</td>\n",
       "      <td>-18.05</td>\n",
       "      <td>-177.90</td>\n",
       "      <td>FIJI ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>08:08:59</td>\n",
       "      <td>56.13</td>\n",
       "      <td>163.72</td>\n",
       "      <td>NEAR EAST COAST OF KAMCHATKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>07:52:08</td>\n",
       "      <td>-27.66</td>\n",
       "      <td>-67.81</td>\n",
       "      <td>CATAMARCA PROVINCE, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>07:37:21</td>\n",
       "      <td>1.15</td>\n",
       "      <td>126.44</td>\n",
       "      <td>NORTHERN MOLUCCA SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>06:56:38</td>\n",
       "      <td>-20.93</td>\n",
       "      <td>-178.52</td>\n",
       "      <td>FIJI ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>05:55:51</td>\n",
       "      <td>0.81</td>\n",
       "      <td>131.70</td>\n",
       "      <td>IRIAN JAYA REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>05:43:31</td>\n",
       "      <td>-18.05</td>\n",
       "      <td>-178.31</td>\n",
       "      <td>FIJI ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>05:03:27</td>\n",
       "      <td>-30.73</td>\n",
       "      <td>-71.84</td>\n",
       "      <td>NEAR COAST OF CENTRAL CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>04:33:09</td>\n",
       "      <td>-5.68</td>\n",
       "      <td>154.34</td>\n",
       "      <td>SOLOMON ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>02:49:43</td>\n",
       "      <td>30.12</td>\n",
       "      <td>-42.69</td>\n",
       "      <td>NORTHERN MID-ATLANTIC RIDGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>02:20:08</td>\n",
       "      <td>2.36</td>\n",
       "      <td>126.72</td>\n",
       "      <td>NORTHERN MOLUCCA SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25-MAY-2024</td>\n",
       "      <td>00:10:13</td>\n",
       "      <td>3.71</td>\n",
       "      <td>94.86</td>\n",
       "      <td>OFF W COAST OF NORTHERN SUMATRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24-MAY-2024</td>\n",
       "      <td>22:51:21</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>134.98</td>\n",
       "      <td>IRIAN JAYA REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24-MAY-2024</td>\n",
       "      <td>18:59:32</td>\n",
       "      <td>51.97</td>\n",
       "      <td>173.88</td>\n",
       "      <td>NEAR ISLANDS, ALEUTIAN ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24-MAY-2024</td>\n",
       "      <td>17:35:24</td>\n",
       "      <td>39.32</td>\n",
       "      <td>17.06</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24-MAY-2024</td>\n",
       "      <td>17:07:12</td>\n",
       "      <td>24.20</td>\n",
       "      <td>121.72</td>\n",
       "      <td>TAIWAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24-MAY-2024</td>\n",
       "      <td>17:06:37</td>\n",
       "      <td>24.16</td>\n",
       "      <td>121.72</td>\n",
       "      <td>TAIWAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Fecha      Hora Latitud Longitud                           Región\n",
       "0   25-MAY-2024  12:02:47  -37.96   -74.00       OFF COAST OF CENTRAL CHILE\n",
       "1   25-MAY-2024  11:52:45   -4.79   133.90     IRIAN JAYA REGION, INDONESIA\n",
       "2   25-MAY-2024  10:16:33   18.98   -65.48               PUERTO RICO REGION\n",
       "3   25-MAY-2024  10:10:39  -18.05  -177.90              FIJI ISLANDS REGION\n",
       "4   25-MAY-2024  08:08:59   56.13   163.72     NEAR EAST COAST OF KAMCHATKA\n",
       "5   25-MAY-2024  07:52:08  -27.66   -67.81    CATAMARCA PROVINCE, ARGENTINA\n",
       "6   25-MAY-2024  07:37:21    1.15   126.44             NORTHERN MOLUCCA SEA\n",
       "7   25-MAY-2024  06:56:38  -20.93  -178.52              FIJI ISLANDS REGION\n",
       "8   25-MAY-2024  05:55:51    0.81   131.70     IRIAN JAYA REGION, INDONESIA\n",
       "9   25-MAY-2024  05:43:31  -18.05  -178.31              FIJI ISLANDS REGION\n",
       "10  25-MAY-2024  05:03:27  -30.73   -71.84      NEAR COAST OF CENTRAL CHILE\n",
       "11  25-MAY-2024  04:33:09   -5.68   154.34                  SOLOMON ISLANDS\n",
       "12  25-MAY-2024  02:49:43   30.12   -42.69      NORTHERN MID-ATLANTIC RIDGE\n",
       "13  25-MAY-2024  02:20:08    2.36   126.72             NORTHERN MOLUCCA SEA\n",
       "14  25-MAY-2024  00:10:13    3.71    94.86  OFF W COAST OF NORTHERN SUMATRA\n",
       "15  24-MAY-2024  22:51:21   -1.22   134.98     IRIAN JAYA REGION, INDONESIA\n",
       "16  24-MAY-2024  18:59:32   51.97   173.88   NEAR ISLANDS, ALEUTIAN ISLANDS\n",
       "17  24-MAY-2024  17:35:24   39.32    17.06                   SOUTHERN ITALY\n",
       "18  24-MAY-2024  17:07:12   24.20   121.72                           TAIWAN\n",
       "19  24-MAY-2024  17:06:37   24.16   121.72                           TAIWAN"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url7 = \"http://ds.iris.edu/seismon/eventlist/index.phtml\"\n",
    "earthquake = requests.get(url7)\n",
    "soup = BeautifulSoup(earthquake.content,\"html.parser\")\n",
    "\n",
    "result = []\n",
    "earthquake_table = soup.find('table', class_='tablesorter')\n",
    "earthquake_row = earthquake_table.find_all('tr')\n",
    "for row in earthquake_row[1:21]:\n",
    "    col = row.find_all('td')\n",
    "    fecha = col[0].text.strip().split(\" \")[0]\n",
    "    hora = col[0].text.strip().split(\" \")[1]\n",
    "    latitud = col[1].text.strip()\n",
    "    longitud = col[2].text.strip()\n",
    "    region = col[5].text.strip()\n",
    "    result.append([fecha, hora, latitud, longitud, region])\n",
    "\n",
    "df = pd.DataFrame(result, columns=['Fecha', 'Hora', 'Latitud', 'Longitud','Región'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 11 - Datos del Top 250 de IMDB (nombre de la película, lanzamiento inicial, nombre del director y estrellas) como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mi user-agent es: python-requests/2.31.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Título</th>\n",
       "      <th>Año lanzamiento</th>\n",
       "      <th>Puntuacion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>[1994]</td>\n",
       "      <td>9.3 (2.9M)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>[1972]</td>\n",
       "      <td>9.2 (2M)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>[2008]</td>\n",
       "      <td>9.0 (2.9M)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El padrino parte II</td>\n",
       "      <td>[1974]</td>\n",
       "      <td>9.0 (1.4M)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>[1957]</td>\n",
       "      <td>9.0 (868K)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Sucedió una noche</td>\n",
       "      <td>[1934]</td>\n",
       "      <td>8.1 (113K)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Bailando con lobos</td>\n",
       "      <td>[1990]</td>\n",
       "      <td>8.0 (292K)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Aladdín</td>\n",
       "      <td>[1992]</td>\n",
       "      <td>8.0 (469K)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>[2015]</td>\n",
       "      <td>8.2 (95K)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Gangs of Wasseypur</td>\n",
       "      <td>[2012]</td>\n",
       "      <td>8.2 (104K)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Título Año lanzamiento  Puntuacion\n",
       "0           Cadena perpetua          [1994]  9.3 (2.9M)\n",
       "1                El padrino          [1972]    9.2 (2M)\n",
       "2       El caballero oscuro          [2008]  9.0 (2.9M)\n",
       "3       El padrino parte II          [1974]  9.0 (1.4M)\n",
       "4     12 hombres sin piedad          [1957]  9.0 (868K)\n",
       "..                      ...             ...         ...\n",
       "245       Sucedió una noche          [1934]  8.1 (113K)\n",
       "246      Bailando con lobos          [1990]  8.0 (292K)\n",
       "247                 Aladdín          [1992]  8.0 (469K)\n",
       "248                Drishyam          [2015]   8.2 (95K)\n",
       "249      Gangs of Wasseypur          [2012]  8.2 (104K)\n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url11 = 'https://www.imdb.com/chart/top'\n",
    "\n",
    "pelis = requests.get(url11, headers=headers)\n",
    "soup11 = BeautifulSoup(pelis.text,\"html.parser\")\n",
    "\n",
    "result = []\n",
    "\n",
    "lista = soup11.find('ul', class_='ipc-metadata-list ipc-metadata-list--dividers-between sc-a1e81754-0 eBRbsI compact-list-view ipc-metadata-list--base')\n",
    "items = lista.find_all('li')\n",
    "for item in items:\n",
    "    title = item.find('h3', class_='ipc-title__text').text.split('.')[-1]\n",
    "    ano_lanzamiento = item.find('span', class_='sc-b189961a-8')\n",
    "    rating = item.find('span', class_= 'ipc-rating-star ipc-rating-star--base ipc-rating-star--imdb ratingGroup--imdb-rating').text\n",
    "    result.append([title, ano_lanzamiento, rating])\n",
    "\n",
    "df = pd.DataFrame(result, columns=['Título','Año lanzamiento', 'Puntuacion'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 12 - Nombre de la película, año y un breve resumen de las 10 películas aleatorias top (IMDB) como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 13 - Encontrar el reporte meteorológico en vivo (temperatura, velocidad del viento, descripción y clima) de una ciudad dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather information:\n",
      "City: Molins de Rei, Country: ES\n",
      "Temperature: 22.97°C\n",
      "Weather Description: clear sky\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(api_key, city):\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "    city_name = city\n",
    "    complete_url = f\"{base_url}appid={api_key}&q={city_name}\"\n",
    "    response = requests.get(complete_url)\n",
    "    return response.json()\n",
    "\n",
    "def format_weather_data(weather_data):\n",
    "    if 'main' in weather_data and 'temp' in weather_data['main']:\n",
    "        temperature = weather_data['main']['temp']\n",
    "        temperature_celsius = temperature - 273.15  # Convertir de Kelvin a Celsius\n",
    "        weather_description = weather_data['weather'][0]['description']\n",
    "        city_name = weather_data['name']\n",
    "        country_code = weather_data['sys']['country']\n",
    "        formatted_data = f\"City: {city_name}, Country: {country_code}\\n\"\n",
    "        formatted_data += f\"Temperature: {temperature_celsius:.2f}°C\\n\"\n",
    "        formatted_data += f\"Weather Description: {weather_description}\"\n",
    "        return formatted_data\n",
    "    else:\n",
    "        return \"No se pudo obtener la información del clima.\"\n",
    "\n",
    "# Rellenando la info\n",
    "api_key = '4ab4a4e8fe492f94623e26ebf76139c4'\n",
    "city_name = 'Molins de Rei'\n",
    "weather_data = get_weather(api_key, city_name)\n",
    "\n",
    "# Formatear y mostrar la información del clima\n",
    "if weather_data:\n",
    "    formatted_weather_data = format_weather_data(weather_data)\n",
    "    print(\"Weather information:\")\n",
    "    print(formatted_weather_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 14 - Nombre del libro, precio y disponibilidad de stock como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url14 = 'http://books.toscrape.com/'\n",
    "books = requests.get(url11)\n",
    "soup = BeautifulSoup(books.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "libros = list(soup.select('h1.h3 a[tittle]'))\n",
    "#nombres = soup.select('h1.h3 a[tittle]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre del Libro</th>\n",
       "      <th>Precio</th>\n",
       "      <th>Disponibilidad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets of Getting Your Dream...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A Novel Based on the Life of...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the Boat: Nine Americans and Their...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade Trilogy, #1)</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little Life (Scott Pi...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and Start Again</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be Your Life: Scenes from the A...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science Fiction Stories 18...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Nombre del Libro  Precio Disponibilidad\n",
       "0                                A Light in the Attic  £51.77       In stock\n",
       "1                                  Tipping the Velvet  £53.74       In stock\n",
       "2                                          Soumission  £50.10       In stock\n",
       "3                                       Sharp Objects  £47.82       In stock\n",
       "4               Sapiens: A Brief History of Humankind  £54.23       In stock\n",
       "5                                     The Requiem Red  £22.65       In stock\n",
       "6   The Dirty Little Secrets of Getting Your Dream...  £33.34       In stock\n",
       "7   The Coming Woman: A Novel Based on the Life of...  £17.93       In stock\n",
       "8   The Boys in the Boat: Nine Americans and Their...  £22.60       In stock\n",
       "9                                     The Black Maria  £52.15       In stock\n",
       "10     Starving Hearts (Triangular Trade Trilogy, #1)  £13.99       In stock\n",
       "11                              Shakespeare's Sonnets  £20.66       In stock\n",
       "12                                        Set Me Free  £17.46       In stock\n",
       "13  Scott Pilgrim's Precious Little Life (Scott Pi...  £52.29       In stock\n",
       "14                          Rip it Up and Start Again  £35.02       In stock\n",
       "15  Our Band Could Be Your Life: Scenes from the A...  £57.25       In stock\n",
       "16                                               Olio  £23.88       In stock\n",
       "17  Mesaerion: The Best Science Fiction Stories 18...  £37.59       In stock\n",
       "18                       Libertarianism for Beginners  £51.33       In stock\n",
       "19                            It's Only the Himalayas  £45.17       In stock"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_containers = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "#  listas para almacenar los datos\n",
    "titles = []\n",
    "prices = []\n",
    "availabilities = []\n",
    "\n",
    "    \n",
    "for container in book_containers:\n",
    "        # sacamos titulo\n",
    "        title = container.h3.a['title']\n",
    "        titles.append(title)\n",
    "\n",
    "        # sacamos precio \n",
    "        price = container.find('p', class_='price_color').text\n",
    "        prices.append(price)\n",
    "\n",
    "        # sacamos disponibilidad\n",
    "        availability = container.find('p', class_='instock availability').text.strip()\n",
    "        availabilities.append(availability)\n",
    "\n",
    "    #\n",
    "df_books = pd.DataFrame({\n",
    "        'Nombre del Libro': titles,\n",
    "        'Precio': prices,\n",
    "        'Disponibilidad': availabilities\n",
    "    })\n",
    "\n",
    "df_books\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitates tu output? Gracias! 🙂**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
